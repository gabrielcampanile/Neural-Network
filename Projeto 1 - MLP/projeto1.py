# -*- coding: utf-8 -*-
"""Projeto1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P82ZHzUfd5w-2c96vUAFcuRgFpQ8iPkn
"""

# === 1. Imports e leitura de dados ===
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import confusion_matrix, accuracy_score

# === 2. Carregamento e pr칠-processamento ===
dataset = pd.read_csv('Matches.csv')
features = [
    'HomeElo', 'AwayElo',
    'Form3Home', 'Form3Home',
    'Form5Away', 'Form5Away',
    'OddHome', 'OddDraw', 'OddAway',
    'HandiSize',
    # 'HTHome', 'HTAway',
    ]
df = dataset[features + ['FTResult']].dropna().reset_index(drop=True)
X = StandardScaler().fit_transform(df[features])
y = LabelEncoder().fit_transform(df['FTResult'])

# === 3. Divis칚o dos dados e convers칚o para Tensor ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_test = torch.tensor(X_train, dtype=torch.float32).to(device), torch.tensor(X_test, dtype=torch.float32).to(device)
y_train, y_test = torch.tensor(y_train, dtype=torch.long).to(device), torch.tensor(y_test, dtype=torch.long).to(device)

# Rebalanceamento das classes com peso
unique, counts = np.unique(y_train.cpu().numpy(), return_counts=True)
print(dict(zip(unique, counts)))
class_counts = np.bincount(y_train.cpu().numpy())
weights = 1.0 / torch.tensor(class_counts, dtype=torch.float32).to(device)

# === 4. Modelo ===
class MLP(nn.Module):
    def __init__(self, input_size, hidden_sizes=[], output_size=3):
        super(MLP, self).__init__()
        layers = []
        in_features = input_size
        for h in hidden_sizes:
            layers.append(nn.Linear(in_features, h))
            layers.append(nn.ReLU())
            in_features = h
        layers.append(nn.Linear(in_features, output_size))
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)

# === 5. Fun칞칚o de treinamento ===
def train(model, X_train, y_train, X_test, y_test, epochs=100, lr=0.01, momentum=0):
    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)
    loss_fn = nn.CrossEntropyLoss(weight=weights)
    # loss_fn = nn.CrossEntropyLoss()
    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        preds = model(X_train)
        loss = loss_fn(preds, y_train)
        loss.backward()
        optimizer.step()

        model.eval()
        with torch.no_grad():
            val_preds = model(X_test)
            val_loss = loss_fn(val_preds, y_test)
            val_acc = accuracy_score(y_test.cpu(), torch.argmax(val_preds, dim=1).cpu())

        history['train_loss'].append(loss.item())
        history['val_loss'].append(val_loss.item())
        history['val_acc'].append(val_acc)

        if epoch % 100 == 0:
            print(f"Epoch {epoch} - Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f} | Val Acc: {val_acc:.4f}")

    return history

# === 6. Avalia칞칚o ===
def evaluate(model, X_test, y_test):
    model.eval()
    with torch.no_grad():
        preds = torch.argmax(model(X_test), dim=1)
    acc = accuracy_score(y_test.cpu(), preds.cpu())
    cm = confusion_matrix(y_test.cpu(), preds.cpu())
    print(f"Acur치cia: {acc * 100:.2f}%")
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=[0,1,2], yticklabels=[0,1,2])
    plt.xlabel("Predito"); plt.ylabel("Real")
    plt.title("Matriz de Confus칚o")
    plt.show()

# === 7. Treinamento com diferentes arquiteturas ===
architectures = [
    [],
    [10],
    [20, 10],
    [40, 20, 10],
    [80, 40, 20, 10]
]

histories = []
accuracies = []

for i, arch in enumerate(architectures):
    print(f"\n== Arquitetura {i+1}: {arch} ==")
    model = MLP(input_size=X_train.shape[1], hidden_sizes=arch).to(device)
    history = train(model, X_train, y_train, X_test, y_test, epochs=500, lr=0.1)
    evaluate(model, X_test, y_test)
    histories.append((f"Arch {i+1}: {arch}", history))
    accuracies.append(history['val_acc'][-1])

# 游늷 Identifique a melhor arquitetura:
best_idx = np.argmax(accuracies)
print(f"\nMelhor arquitetura sem momentum: {architectures[best_idx]} (Acur치cia: {accuracies[best_idx]*100:.2f}%)")

# 游대 Treine novamente a melhor arquitetura com momentum:
print("\n== Repetindo melhor arquitetura COM momentum ==")
best_arch = architectures[best_idx]
model_momentum = MLP(input_size=X_train.shape[1], hidden_sizes=best_arch).to(device)
history_momentum = train(model_momentum, X_train, y_train, X_test, y_test, epochs=500, lr=0.1, momentum=0.9)
evaluate(model_momentum, X_test, y_test)

# 游늳 Plote os gr치ficos:
def plot_metrics(histories, history_momentum=None):
    plt.figure(figsize=(14, 5))

    # Perda
    plt.subplot(1, 2, 1)
    for name, hist in histories:
        plt.plot(hist['val_loss'], label=name)
    if history_momentum:
        plt.plot(history_momentum['val_loss'], label='Melhor + Momentum', linestyle='--', linewidth=2)
    plt.title("Val Loss por 칄poca")
    plt.xlabel("칄poca")
    plt.ylabel("Loss")
    plt.legend()

    # Acur치cia
    plt.subplot(1, 2, 2)
    for name, hist in histories:
        plt.plot(hist['val_acc'], label=name)
    if history_momentum:
        plt.plot(history_momentum['val_acc'], label='Melhor + Momentum', linestyle='--', linewidth=2)
    plt.title("Val Acur치cia por 칄poca")
    plt.xlabel("칄poca")
    plt.ylabel("Acur치cia")
    plt.legend()

    plt.tight_layout()
    plt.show()

plot_metrics(histories, history_momentum)