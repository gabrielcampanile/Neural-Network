{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielcampanile/Neural-Network/blob/main/Projeto%20Final%20-%20Transformers/GRU_translator_EN_PT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tradutor Inglês-Português com Redes Sequência-a-Sequência e Atenção\n",
        "\n",
        "Este notebook implementa um modelo de tradução automática de inglês para português, baseado no tutorial \"NLP From Scratch: Translation with a Sequence to Sequence Network and Attention\" do PyTorch.\n",
        "\n",
        "A arquitetura utilizada é um modelo **Sequence-to-Sequence (Seq2Seq)** composto por um **Encoder** e um **Decoder** com redes neurais recorrentes (GRU). Para melhorar a performance, implementamos um **Mecanismo de Atenção (Attention)**, que permite ao modelo focar em partes específicas da frase de entrada ao gerar a tradução.\n",
        "\n",
        "**Estratégia Principal:** Para garantir que o treinamento seja computacionalmente viável e rápido, seguimos a orientação de limitar o tamanho do nosso dataset. Em vez de usar todas as frases disponíveis, criamos um vocabulário com as palavras mais frequentes e filtramos o dataset para usar apenas um número limitado de frases que podem ser formadas com esse vocabulário.\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "NTnVUPlcVUr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports principais\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "\n",
        "# Imports do PyTorch e utilidades\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "# Imports para plotagem\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# Configuração do dispositivo (GPU ou CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "# Tokens especiais para marcar início e fim de sentenças\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n"
      ],
      "metadata": {
        "id": "P36Ivcu6Vg1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "957bdc94-c90b-482d-9d11-efe87f027615"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparação dos Dados: Classes e Funções de Normalização\n",
        "\n",
        "Para trabalhar com texto, primeiro precisamos processá-lo.\n",
        "\n",
        "1.  **Classe `Lang`**: Uma classe auxiliar para criar um vocabulário. Ela mapeia cada palavra única para um índice numérico (e vice-versa) e conta a frequência de cada palavra.\n",
        "2.  **Normalização de Strings**: Funções para converter o texto para um formato padrão: minúsculas, remoção de acentos e de caracteres especiais. Isso reduz a complexidade do vocabulário.\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "asfIFZQ2VpHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokens especiais atualizados\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK_token = 2 # Novo token para palavras desconhecidas\n",
        "\n",
        "# Classe Lang atualizada para incluir UNK\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"<UNK>\"}\n",
        "        self.n_words = 3\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "# Converte uma string Unicode para ASCII puro\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Deixa em minúsculo, remove espaços e caracteres não-letra\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s"
      ],
      "metadata": {
        "id": "En4yDZRMVvWU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leitura e Filtragem de Dados\n",
        "\n",
        "Aqui definimos as funções para carregar, processar e preparar o dataset para o treinamento. Esta é a etapa mais customizada do nosso projeto.\n",
        "\n",
        "1.  `readLangs`: Lê o arquivo de texto `eng-por.txt` e o divide em pares de sentenças.\n",
        "2.  `prepareDataWithVocabLimit`: Nossa função principal de preparação. Ela lê todos os pares, conta a frequência das palavras, cria vocabulários com as `N` palavras mais comuns, e então filtra o dataset para manter apenas um número limitado de frases curtas que usam exclusivamente palavras desses vocabulários.\n",
        "3.  `get_dataloader`: Junta tudo, executa a preparação e cria um `DataLoader` do PyTorch, que nos ajudará a alimentar o modelo com dados em lotes (batches) de forma eficiente.\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "568AYrZ_V8VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Lendo as linhas...\")\n",
        "    # Assumimos que o arquivo limpo está em 'data/eng-por.txt'\n",
        "    lines = open(f'data/{lang1}-{lang2}.txt', encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    pairs = []\n",
        "    for l in lines:\n",
        "      line_parts = l.split('\\t')\n",
        "      if len(line_parts) == 2:\n",
        "        pairs.append([normalizeString(s) for s in line_parts])\n",
        "\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "def prepareDataWithVocabLimit(lang1, lang2, reverse=False, max_vocab_size=4000, max_pairs=None):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(f\"Lidos {len(pairs)} pares de sentenças\")\n",
        "\n",
        "    temp_input_lang = Lang(input_lang.name)\n",
        "    temp_output_lang = Lang(output_lang.name)\n",
        "    for pair in pairs:\n",
        "        temp_input_lang.addSentence(pair[0])\n",
        "        temp_output_lang.addSentence(pair[1])\n",
        "    print(\"Contagem de palavras completa.\")\n",
        "\n",
        "    sorted_input_vocab = sorted(temp_input_lang.word2count.items(), key=lambda item: item[1], reverse=True)\n",
        "    sorted_output_vocab = sorted(temp_output_lang.word2count.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "    input_lang = Lang(input_lang.name)\n",
        "    output_lang = Lang(output_lang.name)\n",
        "    for word, _ in sorted_input_vocab[:max_vocab_size-2]:\n",
        "        input_lang.addWord(word)\n",
        "    for word, _ in sorted_output_vocab[:max_vocab_size-2]:\n",
        "        output_lang.addWord(word)\n",
        "    print(f\"Vocabulário limitado criado. Input: {input_lang.n_words} palavras, Output: {output_lang.n_words} palavras.\")\n",
        "\n",
        "    filtered_pairs = []\n",
        "    for pair in pairs:\n",
        "        if len(pair[0].split(' ')) >= MAX_LENGTH or len(pair[1].split(' ')) >= MAX_LENGTH:\n",
        "            continue\n",
        "        input_valid = all(word in input_lang.word2index for word in pair[0].split(' '))\n",
        "        output_valid = all(word in output_lang.word2index for word in pair[1].split(' '))\n",
        "        if input_valid and output_valid:\n",
        "            filtered_pairs.append(pair)\n",
        "\n",
        "    if max_pairs:\n",
        "        random.shuffle(filtered_pairs)\n",
        "        filtered_pairs = filtered_pairs[:max_pairs]\n",
        "\n",
        "    print(f\"Dataset final com {len(filtered_pairs)} pares de sentenças.\")\n",
        "    return input_lang, output_lang, filtered_pairs\n",
        "\n",
        "def get_dataloader(batch_size, lang1, lang2, device, max_vocab_size=4000, max_pairs=None, reverse=True):\n",
        "    input_lang, output_lang, pairs = prepareDataWithVocabLimit(lang1, lang2, reverse, max_vocab_size, max_pairs)\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = [input_lang.word2index[word] for word in inp.split(' ')]\n",
        "        tgt_ids = [output_lang.word2index[word] for word in tgt.split(' ')]\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                               torch.LongTensor(target_ids).to(device))\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    return input_lang, output_lang, train_dataloader, pairs"
      ],
      "metadata": {
        "id": "lpwQtCN9WBNt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Arquitetura do Modelo: Seq2Seq com Atenção\n",
        "\n",
        "#### Encoder\n",
        "\n",
        "O `EncoderRNN` processa a frase de entrada e a comprime em um vetor de contexto. Ele usa uma camada de *Embedding* para transformar os índices das palavras em vetores densos, e uma camada *GRU* para processar a sequência.\n",
        "\n",
        "#### Attention Decoder\n",
        "\n",
        "O `AttnDecoderRNN` gera a frase de saída. A cada passo, ele usa o mecanismo `BahdanauAttention` para calcular pesos de atenção, criando um vetor de contexto ponderado que foca nas partes mais relevantes da entrada. Este contexto é combinado com a palavra anterior para prever a próxima palavra da tradução.\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "OmrLACVrWHnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "        return context, weights\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden, attn_weights"
      ],
      "metadata": {
        "id": "tD_bvlMPWUD-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento do Modelo\n",
        "\n",
        "Definimos agora as funções para treinar nosso modelo.\n",
        "\n",
        "  - `train_epoch`: Executa uma única época de treinamento. Para cada lote de dados, ela passa as frases pelo encoder e decoder, calcula a perda (o quão errada a previsão foi) e atualiza os pesos das redes usando os otimizadores.\n",
        "  - `train`: Orquestra o processo de treinamento completo por várias épocas, chamando `train_epoch` repetidamente. Ela também monitora a perda, exibe o progresso e o tempo decorrido.\n",
        "  - `asMinutes` e `timeSince`: Funções auxiliares para formatar o tempo de treinamento.\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "lcGV-iU4WX10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "metadata": {
        "id": "0os6Zie9WaMu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, print_every=100, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0\n",
        "    plot_loss_total = 0\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print(\"=\" * 50)\n",
        "            print(f\"⏳ Tempo decorrido: {timeSince(start, epoch / n_epochs)}\")\n",
        "            print(f\"📅 Época: {epoch}/{n_epochs} ({epoch / n_epochs * 100:.2f}%)\")\n",
        "            print(f\"📉 Loss médio: {print_loss_avg:.4f}\")\n",
        "            print(\"=\" * 50 + \"\\n\")\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "metadata": {
        "id": "hELx7ooxW01V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotando os Resultados\n",
        "\n",
        "Uma função simples para plotar a perda (loss) ao longo do treinamento. Ver a perda diminuir é um bom sinal de que o modelo está aprendendo.\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "dua2iVhuW9jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    plt.xlabel(\"Épocas (x100)\")\n",
        "    plt.ylabel(\"Perda (Loss)\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "U9jmJf_tW-QE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação e Visualização\n",
        "\n",
        "Após o treinamento, precisamos de funções para testar nosso modelo.\n",
        "\n",
        "  - `evaluate`: Recebe uma frase em português, a traduz para o inglês e retorna a tradução.\n",
        "  - `evaluateRandomly`: Pega `n` frases aleatórias do nosso dataset de teste, exibe a entrada, o alvo (tradução correta) e a saída do modelo (tradução prevista). É ótima para uma avaliação qualitativa rápida.\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "zwNv9jS4XC1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        # Converte a sentença de entrada em um tensor de índices.\n",
        "        # Se uma palavra não for encontrada, usa o UNK_token.\n",
        "        input_indices = [input_lang.word2index.get(word, UNK_token) for word in sentence.split(' ')]\n",
        "        input_indices.append(EOS_token)\n",
        "        input_tensor = torch.tensor(input_indices, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            # O output não precisa de UNK, pois ele sempre gera palavras de seu vocabulário\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "\n",
        "    return decoded_words, decoder_attn\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, pairs, n=20):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "metadata": {
        "id": "hszxcvZwXHU3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Executando o Treinamento\n",
        "\n",
        "Tudo pronto\\! Nesta célula, definimos os hiperparâmetros finais (tamanho da camada oculta, tamanho do lote, etc.), chamamos nossa função `get_dataloader` para preparar os dados e, finalmente, iniciamos o treinamento com a função `train`.\n",
        "\n",
        "Após o término do treinamento, mudamos os modelos para o modo de avaliação (`.eval()`) e chamamos `evaluateRandomly` para ver alguns resultados.\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "-NBlokUEXK71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Bloco de código para iniciar o treinamento ---\n",
        "\n",
        "# Defina os hiperparâmetros\n",
        "hidden_size = 256\n",
        "batch_size = 32\n",
        "MAX_LENGTH = 10        # Frases com até 10 palavras (incluindo pontuação)\n",
        "MAX_VOCAB_SIZE = 4000  # Limite de 4000 palavras mais comuns\n",
        "MAX_PAIRS = 20000      # Limite de 20.000 frases para treinar\n",
        "\n",
        "# Carrega os dados usando a nossa função customizada\n",
        "# lang1='eng', lang2='por', reverse=True -> para traduzir de POR para ENG\n",
        "# A variável 'pairs' é retornada para ser usada na avaliação\n",
        "input_lang, output_lang, train_dataloader, pairs = get_dataloader(\n",
        "    batch_size,\n",
        "    'por',\n",
        "    'eng',\n",
        "    device,\n",
        "    max_vocab_size=MAX_VOCAB_SIZE,\n",
        "    max_pairs=MAX_PAIRS,\n",
        "    reverse=False\n",
        ")\n",
        "\n",
        "# Inicializa os modelos\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "# Inicia o treinamento\n",
        "train(train_dataloader, encoder, decoder, n_epochs=40, print_every=5, plot_every=5)\n"
      ],
      "metadata": {
        "id": "5uID2w7FXnYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f69afd-9de8-4a93-ca8a-594cb37680b7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lendo as linhas...\n",
            "Lidos 196350 pares de sentenças\n",
            "Contagem de palavras completa.\n",
            "Vocabulário limitado criado. Input: 1001 palavras, Output: 1001 palavras.\n",
            "Dataset final com 10000 pares de sentenças.\n",
            "==================================================\n",
            "⏳ Tempo decorrido: 0m 36s (- 4m 13s)\n",
            "📅 Época: 5/40 (12.50%)\n",
            "📉 Loss médio: 1.1959\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "⏳ Tempo decorrido: 1m 13s (- 3m 39s)\n",
            "📅 Época: 10/40 (25.00%)\n",
            "📉 Loss médio: 0.2329\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "⏳ Tempo decorrido: 1m 49s (- 3m 2s)\n",
            "📅 Época: 15/40 (37.50%)\n",
            "📉 Loss médio: 0.0977\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "⏳ Tempo decorrido: 2m 25s (- 2m 25s)\n",
            "📅 Época: 20/40 (50.00%)\n",
            "📉 Loss médio: 0.0614\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "⏳ Tempo decorrido: 3m 2s (- 1m 49s)\n",
            "📅 Época: 25/40 (62.50%)\n",
            "📉 Loss médio: 0.0439\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "⏳ Tempo decorrido: 3m 38s (- 1m 12s)\n",
            "📅 Época: 30/40 (75.00%)\n",
            "📉 Loss médio: 0.0380\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "⏳ Tempo decorrido: 4m 15s (- 0m 36s)\n",
            "📅 Época: 35/40 (87.50%)\n",
            "📉 Loss médio: 0.0330\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "⏳ Tempo decorrido: 4m 52s (- 0m 0s)\n",
            "📅 Época: 40/40 (100.00%)\n",
            "📉 Loss médio: 0.0306\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Avalia resultados aleatórios\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder, pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kYNHArDRWZ-",
        "outputId": "9f337dfe-f9b3-498f-dcdc-bfb16ed687fd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> alguem roubou a carteira de tom .\n",
            "= somebody stole tom s wallet .\n",
            "< somebody stole tom s wallet . <EOS>\n",
            "\n",
            "> eu farei qualquer coisa para ajudar .\n",
            "= i ll do anything to help .\n",
            "< i ll do anything to help . <EOS>\n",
            "\n",
            "> voce vai ficar bem .\n",
            "= you re going to be ok .\n",
            "< you re going to be ok . <EOS>\n",
            "\n",
            "> nos podemos apenas ir para casa ?\n",
            "= can we just go home ?\n",
            "< can we just go home ? <EOS>\n",
            "\n",
            "> por favor venha a minha casa .\n",
            "= please come to my house .\n",
            "< please come to my house . <EOS>\n",
            "\n",
            "> eu nao vou parar agora .\n",
            "= i m not going to stop now .\n",
            "< i m not going to stop now . <EOS>\n",
            "\n",
            "> tom nao o entende .\n",
            "= tom doesn t get it .\n",
            "< tom doesn t get it . <EOS>\n",
            "\n",
            "> tom ja tem um plano .\n",
            "= tom already has a plan .\n",
            "< tom already has a plan . <EOS>\n",
            "\n",
            "> tom decidiu fazer aquilo .\n",
            "= tom has decided to do that .\n",
            "< tom has decided to do that . <EOS>\n",
            "\n",
            "> o que o medico te disse ?\n",
            "= what did the doctor tell you ?\n",
            "< what did the doctor tell you ? <EOS>\n",
            "\n",
            "> eles tem a mesma idade .\n",
            "= they re the same age .\n",
            "< they re the same age . <EOS>\n",
            "\n",
            "> eu tive de fazer aquilo sozinho .\n",
            "= i had to do that by myself .\n",
            "< i had to do that by myself . <EOS>\n",
            "\n",
            "> eu nao o teria dito assim .\n",
            "= i wouldn t have said it like that .\n",
            "< i wouldn t have said it like that . <EOS>\n",
            "\n",
            "> tudo bem se sentar aqui ?\n",
            "= is it ok to sit here ?\n",
            "< is it ok to sit here ? <EOS>\n",
            "\n",
            "> voce tem um pouco de agua ?\n",
            "= do you have any water ?\n",
            "< do you have any water ? <EOS>\n",
            "\n",
            "> muitas pessoas nao sabem nadar .\n",
            "= many people don t know how to swim .\n",
            "< many people don t know how to swim . <EOS>\n",
            "\n",
            "> isso seria legal nao era ?\n",
            "= that would be nice wouldn t it ?\n",
            "< that would be nice wouldn t it ? <EOS>\n",
            "\n",
            "> posso falar uma coisa ?\n",
            "= can i say one thing ?\n",
            "< can i say one thing ? <EOS>\n",
            "\n",
            "> qual e o problema agora ?\n",
            "= what s the matter now ?\n",
            "< what s the matter now ? <EOS>\n",
            "\n",
            "> eu gosto de viajar .\n",
            "= i like to travel .\n",
            "< i like to travel . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Avalia resultados aleatórios\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# Lista de sentenças para traduzir\n",
        "sentences_to_translate = [\n",
        "    \"eu te amo .\",\n",
        "    \"ola como voce esta ?\",\n",
        "    \"eu gosto de programar .\",\n",
        "    \"isto e um teste .\",\n",
        "    \"onde voce mora ?\",\n",
        "    \"qual e o seu nome ?\",\n",
        "    \"como voce esta hoje ?\"\n",
        "]\n",
        "\n",
        "for sentence in sentences_to_translate:\n",
        "    output_words, _ = evaluate(encoder, decoder, sentence, input_lang, output_lang)\n",
        "    output_sentence = ' '.join(output_words)\n",
        "    print(f'Original: {sentence}')\n",
        "    print(f'Tradução: {output_sentence}')\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwZke0_PUPOV",
        "outputId": "4b9f7e0d-3abe-40e9-8137-c2248810256d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: eu te amo .\n",
            "Tradução: i love you . <EOS>\n",
            "--------------------\n",
            "Original: ola como voce esta ?\n",
            "Tradução: how are you doing ? <EOS>\n",
            "--------------------\n",
            "Original: eu gosto de programar .\n",
            "Tradução: i like it . <EOS>\n",
            "--------------------\n",
            "Original: isto e um teste .\n",
            "Tradução: this is a test . <EOS>\n",
            "--------------------\n",
            "Original: onde voce mora ?\n",
            "Tradução: where do you live near ? <EOS>\n",
            "--------------------\n",
            "Original: qual e o seu nome ?\n",
            "Tradução: what s his name ? <EOS>\n",
            "--------------------\n",
            "Original: como voce esta hoje ?\n",
            "Tradução: how are you today ? <EOS>\n",
            "--------------------\n"
          ]
        }
      ]
    }
  ]
}